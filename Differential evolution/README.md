### Abstract
Differential evolution (DE) is a popular evolutionary algorithm inspired by Darwinâ€™s theory of evolution and has been studied extensively to solve different areas of optimisation and engineering applications since its introduction by Storn in 1997. This study aims to review the massive progress of DE in the research community by analysing the 192 articles published on this subject from 1997 to 2021, particularly studies in the past five years. The methodology used to search for relevant DE papers and an overview of the original DE are firstly explained. Recent advances in the modifications proposed to enhance the effectiveness and efficiency of the original DE are reviewed by analysing the strengths and weaknesses of each published work, followed by the potential applications of these DE variants in solving different real-world engineering problems. In contrast to most existing DE review papers, additional analyses are performed in this survey by investigating the impacts of various parameter settings on given DE variants to identify their optimal values required for solving certain problem classes. The qualities of modifications incorporated into selected DE variants are also evaluated by measuring the performance gains achieved in terms of search accuracy and/or efficiency against the original DE. The additional surveys conducted in this study are anticipated to provide more insightful perspectives for both beginners and experts of DE research, enabling their better understanding about current research trends and new motivations to outline appropriate strategic planning for future development works.

Step 1.1: Define the research area. The research areas are the enhanced technique and applications of DE algorithms to solve various problems, 

![1-s2 0-S111001682100613X-gr2 (1)](https://github.com/yousefbaz12/Deep-Learning-demos/assets/106428761/1db8c4bd-c618-4eb0-88f1-2cdbc1f1358a)

Double verification process for each article. The remaining selected articles were determined if they lie in the areas of focus. In this review, our focused parts were exploitation and exploration processes, convergence rate, computation time, search accuracy and robustness of an algorithm, However, other outstanding achievements of researchers were also included as extra information that can be value-adding to this review paper. The articles were mainly selected from high-impact journals and several book chapters in explaining the basic theory of the DE algorithm. A few conference papers were also included as extra references for a general review of DE algorithms.  the tabulation of publications used in this review. On the basis of the chart, most of the chosen articles were published between 2016 and 2020 because the latest advances of DE and modification techniques proposed in these variants were surveyed. The DE and non-DE algorithms are designated with blue and orange colours, respectively, Some non-DE research papers were included in this review process to provide additional information to readers such as the basic concept of other MSAs, as well as some open research challenges and future research directions that might be applicable to outline the future development plans of DE research.

![1-s2 0-S111001682100613X-gr3](https://github.com/yousefbaz12/Deep-Learning-demos/assets/106428761/6ac92906-f1ff-4558-8cbb-ade240c1febb)

### Explain how to get the optimal solution in metahueristic algoritm

--Define the problem: Clearly define the optimization problem you want to solve, including the objective function, decision variables, constraints, and any specific requirements.

--Choose an appropriate metaheuristic algorithm: There are various metaheuristic algorithms available, such as genetic algorithms, simulated annealing, particle swarm optimization, ant colony optimization, and tabu search. Select an algorithm that suits your problem characteristics and requirements.

--Design the solution representation: Represent the solutions in a suitable format, such as binary strings, real-valued vectors, permutations, or graphs. The representation should allow the algorithm to explore the search space effectively.

--Define the objective function: Establish a measure of the solution quality with an objective function. This function quantifies how well a solution satisfies the problem's goals. Ensure the objective function is compatible with the algorithm's requirements.

--Fine-tune algorithm parameters: Metaheuristic algorithms have several parameters that influence their behavior, such as population size, mutation rate, crossover probability, cooling schedule, or neighborhood size. Experiment with different parameter settings to find the optimal balance between exploration and exploitation.

--Implement the algorithm: Translate the chosen metaheuristic algorithm into a computer program or use existing libraries and frameworks. Pay attention to the details of the algorithm, including initialization, termination criteria, search operators, and stopping conditions.

--Perform multiple runs: Since metaheuristic algorithms are stochastic, the initial conditions and randomness affect the results. Run the algorithm multiple times with different random seeds or starting points to obtain a set of solutions and evaluate their quality.

--Evaluate and compare solutions: Analyze the solutions obtained by the algorithm using appropriate metrics, such as the objective function value, convergence rate, or solution diversity. Compare the obtained solutions to known or benchmark solutions if available.

--Iterate and refine: Based on the evaluation results, refine the algorithm by adjusting parameters, modifying operators, or incorporating problem-specific knowledge. Iterate this process to improve the algorithm's performance gradually.

--Consider advanced techniques: There are advanced techniques that can be combined with metaheuristic algorithms to enhance their performance. These include local search heuristics, problem-specific heuristics, parallelization, hybridization with other algorithms, or algorithmic variations.

--Remember that metaheuristic algorithms do not guarantee the optimal solution but aim to find good solutions efficiently. The success of these algorithms depends on proper problem modeling, algorithm design, parameter tuning, and iterative improvement.

![90e96f00-943f-11eb-8865-f62c4b7b0408 (1)](https://github.com/yousefbaz12/Deep-Learning-demos/assets/106428761/a28b6614-b947-47cf-9a65-665113e63062)





